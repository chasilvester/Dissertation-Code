{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote\n",
    "using JuMP\n",
    "using Ipopt\n",
    "using LinearAlgebra\n",
    "using MathOptInterface\n",
    "const MOI = MathOptInterface\n",
    "using SparseArrays\n",
    "using BenchmarkTools\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feasibility restoration phase function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function restoration_phase(xk, ρ)\n",
    "    println(\"Solving restoration problem\")\n",
    "    \n",
    "    is_s_eq_0 = false\n",
    "    \n",
    "    # Nonlinear model\n",
    "    nlrp = Model()\n",
    "    set_silent(nlrp)\n",
    "    @variable(nlrp, x[i=1:2])\n",
    "    @NLobjective(nlrp, Min, (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2)\n",
    "    @NLconstraint(nlrp, c2, x[1]^2 + x[2]^2 - 2 <= 0)\n",
    "    @NLconstraint(nlrp, c3, -1.5 - x[1] <= 0)\n",
    "    @NLconstraint(nlrp, c4, x[1] - 1.5 <= 0)\n",
    "    @NLconstraint(nlrp, c5, -1.5 - x[2] <= 0)\n",
    "    @NLconstraint(nlrp, c6, x[2] - 1.5 <= 0)\n",
    "    \n",
    "    # Initialise MOI evaluator\n",
    "    pp = NLPEvaluator(nlrp)\n",
    "    MOI.initialize(pp, [:Jac,:Grad,:Hess])\n",
    "\n",
    "    while !is_s_eq_0\n",
    "        \n",
    "        #Constraint eval\n",
    "        ggg=zeros(length(all_nonlinear_constraints(nlrp)))\n",
    "        MOI.eval_constraint(pp,ggg,xk)\n",
    "\n",
    "        #Jacobian eval\n",
    "        JStru=MOI.jacobian_structure(pp)\n",
    "        Ja=zeros(length(JStru))\n",
    "        MOI.eval_constraint_jacobian(pp,Ja,xk)\n",
    "\n",
    "        jr = Vector{Int64}(undef, length(Ja)); jc = Vector{Int64}(undef, length(Ja))\n",
    "        for i in 1:length(JStru)\n",
    "            jr[i] = JStru[i][1] |> Int\n",
    "            jc[i] = JStru[i][2] |> Int\n",
    "        end\n",
    "        Jacobeval = sparse(jr, jc, Ja) \n",
    "        Jacobi = Matrix(Jacobeval)\n",
    "        \n",
    "        restor_model = Model(Ipopt.Optimizer)\n",
    "        set_silent(restor_model)\n",
    "        @variable(restor_model, s >= 0)\n",
    "        @variable(restor_model, d[i=1:2])\n",
    "\n",
    "        @objective(restor_model, Min, s) \n",
    "        @constraint(restor_model, c[i in 1:length(all_nonlinear_constraints(nlrp))],  ggg[i] + dot(Jacobi[i, :], d) - s <= 0)\n",
    "        @constraint(restor_model, c7, d[1] <= ρ)\n",
    "        @constraint(restor_model, c8, d[2] <= ρ) \n",
    "        @constraint(restor_model, c9, d[1] >= -ρ)\n",
    "        @constraint(restor_model, c10, d[2] >= -ρ)\n",
    "        optimize!(restor_model)\n",
    "\n",
    "        proposed_xk = xk .+ value.(d)\n",
    "        \n",
    "        # merit function\n",
    "        function m(x::Vector{<:Real}) \n",
    "            ce = zeros(length(all_nonlinear_constraints(nlrp)))\n",
    "            MOI.eval_constraint(pp,ce,x)\n",
    "            \n",
    "            max_ineq_rows = []\n",
    "            for i in 1:length(ce)\n",
    "                maxval = max(ce[i], 0)\n",
    "                max_ineq_rows = push!(max_ineq_rows, maxval)\n",
    "            end\n",
    "            \n",
    "            merit = norm(max_ineq_rows)\n",
    "            return merit\n",
    "        end\n",
    "\n",
    "        # Update step\n",
    "        if abs(value(s)) < 1e-6\n",
    "        elseif m(proposed_xk) < m(xk) \n",
    "            println(\"restoration merit condition: success\")\n",
    "            xk = proposed_xk\n",
    "            ρ += 1*ρ\n",
    "        else\n",
    "            println(\"restoration merit condition: failed\")\n",
    "            ρ -= 0.5*ρ\n",
    "        end\n",
    "        \n",
    "#         println(\"slack = $(value(s))\")\n",
    "        is_s_eq_0 = abs(value(s)) < 1e-6\n",
    "    end\n",
    "    \n",
    "    return xk, ρ \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLP with a merit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SLP_MF(xk::Vector{<:Real}, ρ, γ)\n",
    "    # ρ and γ are parameters\n",
    "    @assert γ>=0\n",
    "    \n",
    "    has_x_converged = false\n",
    "    ctr = 0\n",
    "    restor_ctr = 0\n",
    "    \n",
    "    # Nonlinear model\n",
    "    nlmod = Model()\n",
    "    set_silent(nlmod)\n",
    "    @variable(nlmod, x[i=1:2])\n",
    "    @NLobjective(nlmod, Min, (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2)\n",
    "    @NLconstraint(nlmod, c2, x[1]^2 + x[2]^2 - 2 <= 0)\n",
    "    @NLconstraint(nlmod, c3, -1.5 - x[1] <= 0)\n",
    "    @NLconstraint(nlmod, c4, x[1] - 1.5 <= 0)\n",
    "    @NLconstraint(nlmod, c5, -1.5 - x[2] <= 0)\n",
    "    @NLconstraint(nlmod, c6, x[2] - 1.5 <= 0)\n",
    "    \n",
    "    # Initialise MOI evaluator\n",
    "    p = NLPEvaluator(nlmod)\n",
    "    MOI.initialize(p, [:Jac,:Grad,:Hess])\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    while has_x_converged != [1, 1]\n",
    "        ctr += 1\n",
    "        \n",
    "        #Objective eval\n",
    "        objEv=MOI.eval_objective(p, xk)\n",
    "\n",
    "        #Objective gradient\n",
    "        df=zeros(length(xk))\n",
    "        MOI.eval_objective_gradient(p,df,xk)\n",
    "\n",
    "        #Constraint eval\n",
    "        gg=zeros(length(all_nonlinear_constraints(nlmod)))\n",
    "        MOI.eval_constraint(p,gg,xk)\n",
    "\n",
    "        #Jacobian eval\n",
    "        JStr=MOI.jacobian_structure(p)\n",
    "        J=zeros(length(JStr))\n",
    "        MOI.eval_constraint_jacobian(p,J,xk)\n",
    "\n",
    "        jr = Vector{Int64}(undef, length(J)); jc = Vector{Int64}(undef, length(J))\n",
    "        for i in 1:length(JStr)\n",
    "            jr[i] = JStr[i][1] |> Int\n",
    "            jc[i] = JStr[i][2] |> Int\n",
    "        end\n",
    "        Jacobeval = sparse(jr, jc, J) \n",
    "        Jacobi = Matrix(Jacobeval)\n",
    "\n",
    "        # Linearised model\n",
    "        model = Model(Ipopt.Optimizer)\n",
    "        set_silent(model)\n",
    "        @variable(model, d[i=1:2])\n",
    "        @objective(model, Min, objEv + df'*d)\n",
    "        @constraint(model, c[i in 1:length(all_nonlinear_constraints(nlmod))],  gg[i] + dot(Jacobi[i, :], d) <= 0)\n",
    "        @constraint(model, c7, d[1] <= ρ)\n",
    "        @constraint(model, c8, d[2] <= ρ) \n",
    "        @constraint(model, c9, d[1] >= -ρ)\n",
    "        @constraint(model, c10, d[2] >= -ρ)\n",
    "        \n",
    "        println(\"Iteration $ctr...\")\n",
    "\n",
    "        optimize!(model)\n",
    "        @show termination_status(model)\n",
    "        @show objective_value(model)\n",
    "        \n",
    "        # updating ρ\n",
    "        proposed_xk = xk .+ value.(d)\n",
    "        \n",
    "        # merit function\n",
    "        function m(x::Vector) \n",
    "            newobjev = MOI.eval_objective(p, x)\n",
    "            ce = zeros(length(all_nonlinear_constraints(nlmod)))\n",
    "            MOI.eval_constraint(p,ce,x)\n",
    "            \n",
    "            max_ineq_rows = []\n",
    "            for i in 1:length(ce)\n",
    "                maxval = max(ce[i], 0)\n",
    "                max_ineq_rows = push!(max_ineq_rows, maxval)\n",
    "            end\n",
    "            \n",
    "            merit = newobjev + γ * norm(max_ineq_rows)\n",
    "            return merit\n",
    "        end\n",
    "        \n",
    "        println(\"m(xk) = $(m(xk))\")\n",
    "        println(\"m(prop_xk) = $(m(proposed_xk))\")\n",
    "        \n",
    "        # Update step\n",
    "        if termination_status(model) == MathOptInterface.LOCALLY_INFEASIBLE\n",
    "        elseif m(proposed_xk) < m(xk)\n",
    "            println(\"merit condition: success\")\n",
    "            xk = proposed_xk\n",
    "            ρ += 1*ρ\n",
    "        else\n",
    "            println(\"merit condition: failed\")\n",
    "            ρ -= 0.5*ρ\n",
    "        end\n",
    "        \n",
    "        println(\"d=$(value.(d))\")\n",
    "        println(\"x$(ctr)=$(xk)\")\n",
    "#         println(\"new ρ: $ρ\")\n",
    "        \n",
    "        #restoration phase:\n",
    "        if termination_status(model) == MathOptInterface.LOCALLY_INFEASIBLE\n",
    "            xk, ρ = restoration_phase(xk, ρ)\n",
    "            restor_ctr += 1\n",
    "        end\n",
    "        \n",
    "        #convergence test\n",
    "        tol = 1e-7\n",
    "        has_x_converged = abs.(value.(d)) .< tol\n",
    "        \n",
    "    end\n",
    "    println(\"no. restoration phases: $(restor_ctr)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLP_MF([100., -100.], 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLP with a filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SLP_F(xk::Vector{<:Real}, ρ)\n",
    "    # ρ is the trust region parameter\n",
    "\n",
    "    has_x_converged = false\n",
    "    ctr = 0\n",
    "    restor_ctr = 0\n",
    "    \n",
    "    filt = []\n",
    "    \n",
    "    # Nonlinear model\n",
    "    nlmod = Model()\n",
    "    set_silent(nlmod)\n",
    "    @variable(nlmod, x[i=1:2])\n",
    "    @NLobjective(nlmod, Min, (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2)\n",
    "    @NLconstraint(nlmod, c2, x[1]^2 + x[2]^2 - 2 <= 0)\n",
    "    @NLconstraint(nlmod, c3, -1.5 - x[1] <= 0)\n",
    "    @NLconstraint(nlmod, c4, x[1] - 1.5 <= 0)\n",
    "    @NLconstraint(nlmod, c5, -1.5 - x[2] <= 0)\n",
    "    @NLconstraint(nlmod, c6, x[2] - 1.5 <= 0)\n",
    "    \n",
    "    #Initialise MOI evaluator\n",
    "    p = NLPEvaluator(nlmod)\n",
    "    MOI.initialize(p, [:Jac,:Grad,:Hess])\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    while has_x_converged != [1, 1]\n",
    "        ctr += 1\n",
    "        \n",
    "        p = NLPEvaluator(nlmod)\n",
    "        MOI.initialize(p, [:Jac,:Grad,:Hess])\n",
    "        \n",
    "        #Objective eval\n",
    "        objEv=MOI.eval_objective(p, xk)\n",
    "\n",
    "        #Objective gradient\n",
    "        df=zeros(length(xk))\n",
    "        MOI.eval_objective_gradient(p,df,xk)\n",
    "\n",
    "        #Constraint eval\n",
    "        gg=zeros(length(all_nonlinear_constraints(nlmod)))\n",
    "        MOI.eval_constraint(p,gg,xk)\n",
    "\n",
    "        #Jacobian eval\n",
    "        JStr=MOI.jacobian_structure(p)\n",
    "        J=zeros(length(JStr))\n",
    "        MOI.eval_constraint_jacobian(p,J,xk)\n",
    "\n",
    "        jr = Vector{Int64}(undef, length(J)); jc = Vector{Int64}(undef, length(J))\n",
    "        for i in 1:length(JStr)\n",
    "            jr[i] = JStr[i][1] |> Int\n",
    "            jc[i] = JStr[i][2] |> Int\n",
    "        end\n",
    "        Jacobeval = sparse(jr, jc, J) \n",
    "        Jacobi = Matrix(Jacobeval)\n",
    "        \n",
    "        # Linearised model\n",
    "        model = Model(Ipopt.Optimizer)\n",
    "        set_silent(model)\n",
    "        @variable(model, d[i=1:2])\n",
    "        @objective(model, Min, objEv + df'*d)\n",
    "        @constraint(model, c[i in 1:length(all_nonlinear_constraints(nlmod))],  gg[i] + dot(Jacobi[i, :], d) <= 0)\n",
    "        @constraint(model, c7, d[1] <= ρ)\n",
    "        @constraint(model, c8, d[2] <= ρ) \n",
    "        @constraint(model, c9, d[1] >= -ρ)\n",
    "        @constraint(model, c10, d[2] >= -ρ)\n",
    "        \n",
    "        println(\"Iteration $ctr...\")\n",
    "\n",
    "        optimize!(model)\n",
    "        @show termination_status(model)\n",
    "        \n",
    "        #functions for making the filter\n",
    "        function fk(x::Vector{<:Real}) \n",
    "            newobjev = MOI.eval_objective(p, x)\n",
    "            return newobjev\n",
    "        end\n",
    "        function hk(x::Vector{<:Real}) \n",
    "            ce = zeros(length(all_nonlinear_constraints(nlmod)))\n",
    "            MOI.eval_constraint(p,ce,x)\n",
    "            \n",
    "            max_ineq_rows = []\n",
    "            for i in 1:length(ce)\n",
    "                maxval = max(ce[i], 0)\n",
    "                max_ineq_rows = push!(max_ineq_rows, maxval)\n",
    "            end\n",
    "            \n",
    "            h = norm(max_ineq_rows)\n",
    "            return h\n",
    "        end\n",
    "        \n",
    "        proposed_xk = xk .+ value.(d)\n",
    "        better_obj = all(v -> fk(proposed_xk) < v[1], filt)\n",
    "        better_constr = all(v -> hk(proposed_xk) < v[2], filt)\n",
    "        \n",
    "        # Update step\n",
    "        if termination_status(model) == MathOptInterface.LOCALLY_INFEASIBLE\n",
    "        elseif better_obj || better_constr\n",
    "            println(\"filter: accept\")\n",
    "            xk = proposed_xk\n",
    "            \n",
    "            # trust region update\n",
    "            ρ += 1*ρ\n",
    "            filt = push!(filt, (fk(xk), hk(xk)))\n",
    "        else\n",
    "            println(\"filter: reject\")\n",
    "            \n",
    "            # trust region update\n",
    "            ρ -= 0.5*ρ\n",
    "#             max_step = norm(value.(d), Inf) # more aggressive trust region update\n",
    "#             ρ = max_step/3\n",
    "            filt = push!(filt, (fk(proposed_xk), hk(proposed_xk)))\n",
    "        end\n",
    "        \n",
    "        \n",
    "        println(\"f(propxk) = $(fk(proposed_xk))\")\n",
    "        println(\"d=$(value.(d))\")\n",
    "        println(\"x$(ctr)=$xk\")  \n",
    "        \n",
    "        #restoration phase:\n",
    "        if termination_status(model) == MathOptInterface.LOCALLY_INFEASIBLE\n",
    "            xk, ρ = restoration_phase(xk, ρ)\n",
    "            restor_ctr += 1\n",
    "        end\n",
    "    \n",
    "        #convergence test\n",
    "        tol = 1e-5\n",
    "        has_x_converged = abs.(value.(d)) .< tol\n",
    "        \n",
    "    end\n",
    "    println(\"no. restoration phases: $(restor_ctr)\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLP_F([100., -100.], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQP with a merit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SQP_MF(xk::Vector{<:Real}, λk::Vector{Float64}, ρ, γ)\n",
    "    # λ, μ are initial lagrangian multipliers\n",
    "    # ρ and γ are parameters\n",
    "\n",
    "    has_x_converged = false\n",
    "    ctr = 0\n",
    "    restor_ctr = 0\n",
    "    \n",
    "    # Nonlinear model\n",
    "    nlmod = Model(Ipopt.Optimizer)\n",
    "    set_silent(nlmod)\n",
    "    @variable(nlmod, x[i=1:2])\n",
    "    @NLobjective(nlmod, Min, (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2)\n",
    "    @NLconstraint(nlmod, c2, x[1]^2 + x[2]^2 - 2 <= 0)\n",
    "    @NLconstraint(nlmod, c3, -1.5 - x[1] <= 0)\n",
    "    @NLconstraint(nlmod, c4, x[1] - 1.5 <= 0)\n",
    "    @NLconstraint(nlmod, c5, -1.5 - x[2] <= 0)\n",
    "    @NLconstraint(nlmod, c6, x[2] - 1.5 <= 0)\n",
    "    p = NLPEvaluator(nlmod)\n",
    "    MOI.initialize(p, [:Jac,:Grad,:Hess])\n",
    "    \n",
    "    \n",
    "    while has_x_converged != [1, 1]\n",
    "        ctr += 1\n",
    "        \n",
    "        p = NLPEvaluator(nlmod)\n",
    "        MOI.initialize(p, [:Jac,:Grad,:Hess])\n",
    "        \n",
    "        #Objective eval\n",
    "        objEv=MOI.eval_objective(p, xk)\n",
    "\n",
    "        #Objective gradient\n",
    "        df=zeros(length(xk))\n",
    "        MOI.eval_objective_gradient(p,df,xk)\n",
    "\n",
    "        #Constraint eval\n",
    "        gg=zeros(length(all_nonlinear_constraints(nlmod)))\n",
    "        MOI.eval_constraint(p,gg,xk)\n",
    "\n",
    "        #Jacobian eval\n",
    "        JStr=MOI.jacobian_structure(p)\n",
    "        J=zeros(length(JStr))\n",
    "        MOI.eval_constraint_jacobian(p,J,xk)\n",
    "\n",
    "        jr = Vector{Int64}(undef, length(J)); jc = Vector{Int64}(undef, length(J))\n",
    "        for i in 1:length(JStr)\n",
    "            jr[i] = JStr[i][1] |> Int\n",
    "            jc[i] = JStr[i][2] |> Int\n",
    "        end\n",
    "        Jacobeval = sparse(jr, jc, J) \n",
    "        Jacobi = Matrix(Jacobeval)\n",
    "        \n",
    "        #Hessian-of-the-Lagrangian eval\n",
    "        HStr=MOI.hessian_lagrangian_structure(p)\n",
    "        H=zeros(length(HStr))\n",
    "        MOI.eval_hessian_lagrangian(p,H,xk,1.0,λk) \n",
    "        \n",
    "        hr = Vector{Int64}(undef, length(H)); hc = Vector{Int64}(undef, length(H))\n",
    "        for i in 1:length(HStr)\n",
    "            hr[i] = HStr[i][1] |> Int\n",
    "            hc[i] = HStr[i][2] |> Int\n",
    "        end\n",
    "        \n",
    "        HessLag = sparse(hr, hc, H)\n",
    "        \n",
    "        hessianobj(d) = 0.5*d'*(HessLag*d)\n",
    "        \n",
    "        # Linear model\n",
    "        model = Model(Ipopt.Optimizer)\n",
    "        set_silent(model)\n",
    "        @variable(model, d[i=1:2])\n",
    "        @objective(model, Min, objEv + df'*d + hessianobj(d)) \n",
    "        @constraint(model, c[i in 1:length(all_nonlinear_constraints(nlmod))],  gg[i] + dot(Jacobi[i, :], d) <= 0)\n",
    "        @constraint(model, c7, d[1] <= ρ)\n",
    "        @constraint(model, c8, d[2] <= ρ) \n",
    "        @constraint(model, c9, d[1] >= -ρ)\n",
    "        @constraint(model, c10, d[2] >= -ρ)\n",
    "        \n",
    "        println(\"Iteration $ctr...\")\n",
    "        \n",
    "        optimize!(model)\n",
    "        @show termination_status(model)\n",
    "        \n",
    "        # update λ\n",
    "        λk = []\n",
    "        for i in 1:length(all_nonlinear_constraints(nlmod))\n",
    "            dual = shadow_price(c[i])\n",
    "            λk = push!(λk, dual)\n",
    "        end\n",
    "        λk = convert(Vector{Float64},λk)\n",
    "        \n",
    "\n",
    "        # merit function\n",
    "        function m(x::Vector{<:Real}) \n",
    "            newobjev = MOI.eval_objective(p, x)\n",
    "            ce = zeros(length(all_nonlinear_constraints(nlmod)))\n",
    "            MOI.eval_constraint(p,ce,x)\n",
    "            \n",
    "            max_ineq_rows = []\n",
    "            for i in 1:length(ce)\n",
    "                maxval = max(ce[i], 0)\n",
    "                max_ineq_rows = push!(max_ineq_rows, maxval)\n",
    "            end\n",
    "            \n",
    "            merit = newobjev + γ * norm(max_ineq_rows)\n",
    "            return merit\n",
    "        end\n",
    "        \n",
    "        # update step\n",
    "        proposed_xk = xk .+ value.(d)\n",
    "        println(\"m(xk) = $(m(xk))\")\n",
    "        println(\"m(prop_xk) = $(m(proposed_xk))\")\n",
    "        \n",
    "        if termination_status(model) == MathOptInterface.LOCALLY_INFEASIBLE\n",
    "        elseif m(proposed_xk) < m(xk)\n",
    "            println(\"merit condition: success\")\n",
    "            xk = proposed_xk\n",
    "            ρ += 1*ρ\n",
    "        else\n",
    "            println(\"merit condition: failed\")\n",
    "            ρ -= 0.5*ρ\n",
    "        end\n",
    "        \n",
    "        println(\"d=$(value.(d))\")\n",
    "        println(\"x$ctr=$xk\")\n",
    "        \n",
    "        #restoration phase:\n",
    "        if termination_status(model) == MathOptInterface.LOCALLY_INFEASIBLE\n",
    "            xk, ρ = restoration_phase(xk, ρ)\n",
    "            restor_ctr += 1\n",
    "        end\n",
    "        \n",
    "        #convergence test\n",
    "        tol = 1e-6\n",
    "        has_x_converged = abs.(value.(d)) .< tol\n",
    "    end\n",
    "    println(\"no. restoration phases: $(restor_ctr)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQP_MF([-10., 20.], [0., 0., 0., 0., 0., 0.], 4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQP with a filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SQP_F(xk::Vector{<:Real}, λk::Vector{Float64}, ρ)\n",
    "    # λ is initial lagrangian multiplier\n",
    "    # ρ is the trust region parameter\n",
    "\n",
    "    has_x_converged = false\n",
    "    ctr = 0\n",
    "    restor_ctr = 0\n",
    "    \n",
    "    filt = []\n",
    "    \n",
    "    # Nonlinear model\n",
    "    nlmod = Model(Ipopt.Optimizer)\n",
    "    @variable(nlmod, x[i=1:2])\n",
    "    @NLobjective(nlmod, Min, (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2)\n",
    "    @NLconstraint(nlmod, c2, x[1]^2 + x[2]^2 - 2 <= 0)\n",
    "    @NLconstraint(nlmod, c3, -1.5 - x[1] <= 0)\n",
    "    @NLconstraint(nlmod, c4, x[1] - 1.5 <= 0)\n",
    "    @NLconstraint(nlmod, c5, -1.5 - x[2] <= 0)\n",
    "    @NLconstraint(nlmod, c6, x[2] - 1.5 <= 0)\n",
    "    p = NLPEvaluator(nlmod)\n",
    "    MOI.initialize(p, [:Jac,:Grad,:Hess])\n",
    "    \n",
    "    \n",
    "    while has_x_converged != [1, 1]\n",
    "        ctr += 1\n",
    "        \n",
    "        p = NLPEvaluator(nlmod)\n",
    "        MOI.initialize(p, [:Jac,:Grad,:Hess])\n",
    "        \n",
    "        #Objective eval\n",
    "        objEv=MOI.eval_objective(p, xk)\n",
    "\n",
    "        #Objective gradient\n",
    "        df=zeros(length(xk))\n",
    "        MOI.eval_objective_gradient(p,df,xk)\n",
    "\n",
    "        #Constraint eval\n",
    "        gg=zeros(length(all_nonlinear_constraints(nlmod)))\n",
    "        MOI.eval_constraint(p,gg,xk)\n",
    "\n",
    "        #Jacobian eval\n",
    "        JStr=MOI.jacobian_structure(p)\n",
    "        J=zeros(length(JStr))\n",
    "        MOI.eval_constraint_jacobian(p,J,xk)\n",
    "\n",
    "        jr = Vector{Int64}(undef, length(J)); jc = Vector{Int64}(undef, length(J))\n",
    "        for i in 1:length(JStr)\n",
    "            jr[i] = JStr[i][1] |> Int\n",
    "            jc[i] = JStr[i][2] |> Int\n",
    "        end\n",
    "        Jacobeval = sparse(jr, jc, J) \n",
    "        Jacobi = Matrix(Jacobeval)\n",
    "        \n",
    "        #Hessian-of-the-Lagrangian eval\n",
    "        HStr=MOI.hessian_lagrangian_structure(p)\n",
    "        H=zeros(length(HStr))\n",
    "        MOI.eval_hessian_lagrangian(p,H,xk,1.0,λk) \n",
    "        \n",
    "        hr = Vector{Int64}(undef, length(H)); hc = Vector{Int64}(undef, length(H))\n",
    "        for i in 1:length(HStr)\n",
    "            hr[i] = HStr[i][1] |> Int\n",
    "            hc[i] = HStr[i][2] |> Int\n",
    "        end\n",
    "        HessLag = sparse(hr, hc, H)\n",
    "        \n",
    "        hessianobj(d) = 0.5*d'*(HessLag*d)\n",
    "        \n",
    "        # Linear model\n",
    "        model = Model(Ipopt.Optimizer)\n",
    "        set_silent(model)\n",
    "        @variable(model, d[i=1:2])\n",
    "        @objective(model, Min, objEv + df'*d + hessianobj(d)) \n",
    "        @constraint(model, c[i in 1:length(all_nonlinear_constraints(nlmod))],  gg[i] + dot(Jacobi[i, :], d) <= 0)\n",
    "        @constraint(model, c7, d[1] <= ρ)\n",
    "        @constraint(model, c8, d[2] <= ρ) \n",
    "        @constraint(model, c9, d[1] >= -ρ)\n",
    "        @constraint(model, c10, d[2] >= -ρ)\n",
    "\n",
    "        optimize!(model)\n",
    "        @show termination_status(model)\n",
    "                \n",
    "        println(\"Iteration $ctr...\")\n",
    "        \n",
    "        # update λ\n",
    "        λk = []\n",
    "        for i in 1:length(all_nonlinear_constraints(nlmod))\n",
    "            dual = shadow_price(c[i])\n",
    "            λk = push!(λk, dual)\n",
    "        end\n",
    "        λk = convert(Vector{Float64},λk)\n",
    "\n",
    "        #functions for making the filter\n",
    "        function fk(x::Vector{<:Real}) \n",
    "            newobjev = MOI.eval_objective(p, x)\n",
    "            return newobjev\n",
    "        end\n",
    "        function hk(x::Vector{<:Real}) \n",
    "            ce = zeros(length(all_nonlinear_constraints(nlmod)))\n",
    "            MOI.eval_constraint(p,ce,x)\n",
    "            \n",
    "            max_ineq_rows = []\n",
    "            for i in 1:length(ce)\n",
    "                maxval = max(ce[i], 0)\n",
    "                max_ineq_rows = push!(max_ineq_rows, maxval)\n",
    "            end\n",
    "            \n",
    "            h = norm(max_ineq_rows)\n",
    "            return h\n",
    "        end\n",
    "        \n",
    "        # update step\n",
    "        proposed_xk = xk .+ value.(d)\n",
    "        better_obj = all(v -> fk(proposed_xk) < v[1], filt)\n",
    "        better_constr = all(v -> hk(proposed_xk) < v[2], filt)\n",
    "        \n",
    "        if termination_status(model) == MathOptInterface.LOCALLY_INFEASIBLE\n",
    "        elseif better_obj || better_constr\n",
    "            println(\"filter: accept\")\n",
    "            xk = proposed_xk\n",
    "            ρ += 1*ρ\n",
    "            filter = push!(filt, (fk(xk), hk(xk)))\n",
    "        else\n",
    "            println(\"filter: reject\")\n",
    "            ρ -= 0.5*ρ\n",
    "            filter = push!(filt, (fk(proposed_xk), hk(proposed_xk)))\n",
    "        end\n",
    "        \n",
    "        \n",
    "        println(\"f(propxk) = $(fk(proposed_xk))\")\n",
    "        println(\"d=$(value.(d))\")\n",
    "        println(\"x$ctr=$xk\")\n",
    "        \n",
    "        #restoration phase:\n",
    "        if termination_status(model) == MathOptInterface.LOCALLY_INFEASIBLE\n",
    "            xk, ρ = restoration_phase(xk, ρ)\n",
    "            restor_ctr += 1\n",
    "        end\n",
    "        \n",
    "        #convergence test\n",
    "        tol = 1e-7\n",
    "        has_x_converged = abs.(value.(d)) .< tol\n",
    "    end\n",
    "    println(\"no. restoration phases: $(restor_ctr)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQP_F([-10., 20.], [0.,0.,0.,0.,0.,0.], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "function rosen_disk(x,y)\n",
    "    if (x)^2 + y^2 > 2\n",
    "        return NaN\n",
    "    end\n",
    "    \n",
    "    return (1-x)^2 + 100*(y - x^2)^2\n",
    "end\n",
    "\n",
    "x = -1.5:0.005:1.5\n",
    "y = -1.5:0.005:1.5\n",
    "\n",
    "rosen_disk_plot = Plots.heatmap(x,y,(x,y)->rosen_disk(x,y),c=:jet, title=\"Disk-Constrained Rosenbrock Function\", xlabel = \"x\", ylabel = \"y\", aspect_ratio=:equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter!(rosen_disk_plot, [1], [1], markershape = :circle, label = \"Global\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
